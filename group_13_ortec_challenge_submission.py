# -*- coding: utf-8 -*-
"""Group 13_Ortec Challenge Submission.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XJJiMVrIuNLBPC7hem8ijVq52H7evH__

# Set up the environment
"""

'''

Goal: 
-Categorizeeach symbol in an invoice in one of 6 categories. The zero category means no classification and the other 5 include: total money
owed, KvK number, IBAN Number and sender.

Implementation:
What we have implemented can be described as follows:
-We did not change the original invoice creator; we create test and validation data using this function.
-We run the baseline neural net thrice, and average the outputs
-We apply a convolution filter over the output, so that small islands (e.g. 500550000, gaps (555505555) and 'weak' edge states (in strings of e.g. 05*5550)
(5* being a weak edge state) have their weights lowered.
-We scale all found falues for 0 (the none-category) with 1.25, to account for the imbalanced ratio of symbols labeled with a 0, 
as compared to those labeled anything else//'false positives' were found to be most abundant: 
if the algorithm finds that it can be a zero, it probably is. Scaling with 1.25 is an experimentally found value.
-The categories that involve specific strings (s.a. IBAN and max value owed) can be searched deterministially. If either of these two
can be found, then we first reset the outputs of said categories, found by machinelearning, and then overwrite.

Results:
No proper quantitative large-scaled analysis has been performed, though the filter, the scaling and the deterministic seem to
consistently clean up the outcome of the neural net. The target 1-5's are pretty much always labeled correctly, 
the target 0's occasionally become victims of false positives. The accuracy on a single invoice can be verified in the last few lines.
'''


#ideas: foresting, boosting

# Setup 
 !wget https://storage.googleapis.com/aibootcamp/data/ortec_templates_updated.zip

!unzip ortec_templates_updated.zip
!rm -r ortec_templates_updated.zip
!rm -r ortec_templates_updated.zip.1
!rm -r ortec_templates_updated.zip.2

# Don't forget to install keras
!pip install -q keras

# Your friendly tokenizer
from keras.preprocessing.text import Tokenizer

# Numpy
import numpy as np

# Invoice data generator
from templates.invoicegen import create_invoice
import inspect

# Pandas
import pandas as pd

# Keras
from keras.models import load_model
from keras import optimizers
from keras.models import Sequential
from keras.layers import SimpleRNN, Dense,Activation, Conv1D, MaxPool1D

## Statistics
from statistics import median

#Re

import re

"""# Load Templates"""

# System hyper parameters here

# How many characters before and after the main char to feed the NN
PADDING = 20  # (for instance: TOTAL:| $ 50000, padding (6) divides the regions before and after the '|')


'''
Ignore:           0
Sender Name:      1 
Sender KVK:       2 
Sender IBAN:      3 
Invoice Reference:4
Total:            5
'''
N_CLASSES = 6

# Create 100 invoices for each template

invoices = []
targets = []

# Load template 1
with open('templates/TEMPLATE_1.txt', 'r') as content_file:
    content = content_file.read()

# Create invoices from template
for i in range(100):
    inv, tar = create_invoice(content)
    invoices.append(inv)
    targets.append(tar)
    
# Load template 2
with open('templates/TEMPLATE_2.txt', 'r') as content_file:
    content = content_file.read()
    
# Create invoices from template
for i in range(100):
    inv, tar = create_invoice(content)
    invoices.append(inv)
    targets.append(tar)

"""## Generate Substrings"""

#Tokenizer means indexing each word.
# Create our tokenizer
# We will tokenize on character level!
# We will NOT remove any characters
tokenizer = Tokenizer(char_level=True, filters=None)
tokenizer.fit_on_texts(invoices)

def gen_sub(inv,tar,pad, m = None):
    '''
    Generates a substring from invoice inv and target list tar 
    using the character at index m as a midpoint.
    
    Params:
    inv - an invoice string
    tar - a target list specifying the type of each item
    pad - the amount of padding to attach before and after the focus character
    
    Returns:
    sub - a string with pad characters, the focus character, pad characters
    '''
    # If no focus character index is set, choose at random
    if m == None:
        m = np.random.randint(0,len(inv))
        
    l = m - pad # define the lower bound of our substring
    h = m + pad + 1 # define the upper (high) of our substring

    # Sometimes, our lower bound could be below zero
    # In this case we attach the remaining characters from the back of the string
    if l < 0:
        # Get the characters from the back of the file
        s1 = inv[l:None]
        
        # Edge case: Sample size larger than string
        # Our upper bound might be higher than the lenth of the text
        # In that case we start from the front again
        if h >= len(inv): 
            # How many characters do we need from the front
            overlap = h - len(inv)
            # The string is the entire invoice + some chars from the front
            s2 = inv
            s_over = inv[None:overlap]
            s2 = s2 + s_over
        else:
            # If we don't need chars from the front 
            # we can just select to the upper bound
            s2 = inv[None:h]
            
        # Create substring
        sub = s1 + s2
        # Ensure the substring has the right length
        assert(len(sub) == pad*2 +1)
        return sub, tar[m]
    
    # Our lower bound might be positive but our upper bound might 
    # still be above the length of the invoice
    elif h >= len(inv):
        # Calc how many chars we need from the front
        overlap = h - len(inv)
        
        # Get string from lower bound to end
        s1 = inv[l:None]
        # Get string from the front of the doc
        s2 = inv[None:overlap]
        sub = s1 + s2
        # Make sure our string has the correct length
        assert(len(sub) == pad*2 +1)
        return sub, tar[m]
    
    # Upper and lower bound lie within the length of the invoice
    else: 
        sub = inv[l:h]
        assert(len(sub) == pad*2 +1)
        return sub, tar[m]

"""## Generate Dataset for Training"""

def gen_dataset(sample_size, n_classes, invoices, targets, tokenizer):
    '''
    Generate a dataset of inputs and outputs for our neural network
    
    Params:
    sample_size - desired sample size
    n_classes - number of classes
    invoices - list of invoices to sample from
    targets - list of corresonding targets to sample from
    tokenizer - a keras tokenizer fit on the invoices
    
    The function creates balanced samples by randomly sampling untill 
    an equal amount of samples of all types is created.
    
    Characters are one hot encoded
    
    Returns:
    x_arr: a numpy array of shape (sample_size, seqence length, number of unique characters)
    y_arr: a numpy array of shape (sample_size,)
    
    
    '''
    
    # Create a budget
    budget = [sample_size / n_classes] * n_classes
    
    # Setup holding variables
    X_train = []
    y_train = []

    # While there is still a budget left...  #this can be done more efficiently. e.g. have fewer samples or something
    while sum(budget) > 0:
        # ... get a random invoice and target list
        index = np.random.randint(0,len(invoices))
        inv = invoices[index]
        tar = targets[index]
        # ... sample up to 10 items from this invoice 
        for j in range(10):
            # Get an item
            x, y = gen_sub(inv,tar,PADDING)
            # if we still have a budget for this items target
            if budget[y] > 0:
                # Tokenize to one hot
                
                xm = tokenizer.texts_to_matrix(x)
                # Add data and target
                X_train.append(xm)
                y_train.append(y)
                budget[y] -= 1
      
    # Create numpy arrays from all data and targets
    x_arr = np.array(X_train)      
    y_arr = np.array(y_train)
    return x_arr,y_arr

# Just some global variables
train_size = 24000
val_size = 240

# shuffle two matrices that have equal len in the same way.
def shuffle_in_unison(a, b):
    assert len(a) == len(b)
    shuffled_a = np.empty(a.shape, dtype=a.dtype)
    shuffled_b = np.empty(b.shape, dtype=b.dtype)
    permutation = np.random.permutation(len(a))
    for old_index, new_index in enumerate(permutation):
        shuffled_a[new_index] = a[old_index]
        shuffled_b[new_index] = b[old_index]
    return shuffled_a, shuffled_b

"""## Model Building"""

def neural_net():
  np.random.seed()

  # A simple model     #Jannes: {First MaxPool then RNN is the way to go}. Convolutional goes over each type of input (the 0-5 thingy). Might make use only of Conv1D's (no RNN)
  model = Sequential()
  model.add(Conv1D(32,2,input_shape=(None, 83))) # The input shape assumes there is 85 possible characters
  model.add(MaxPool1D(2))
  model.add(SimpleRNN(10))
  model.add(Dense(6))
  model.add(Activation('softmax'))
  # sparse_categorical_crossentropy is like categorical crossentropy but without converting targets to one hot
  #adam = keras.optimizers.Adam(beta_1=0.9, beta_2=0.999, epsilon=1e-5, decay=0.1, amsgrad=False). 
  model.compile(loss='sparse_categorical_crossentropy', metrics=['acc'], optimizer='adam')
  
  return model

#Create and compile model
neural_net()

#Not sure if this is defined as bagging
def bagging(x_tr = None, y_tr = None, amountofbags = 3): 
  
  x_pieces_storagebin = []
  y_pieces_storagebin = []

  if (x_tr and y_tr) == None:
    x_tr , y_tr = gen_dataset(train_size, N_CLASSES, invoices, targets, tokenizer)
    x_tr , y_tr = shuffle_in_unison(x_tr,y_tr)
  
  assert len(x_tr) == len(y_tr)
  
  for i in range(0,amountofbags):
    x_pieces_storagebin.append(x_tr[(i*len(x_tr)//amountofbags):(i+1)*(len(x_tr)//amountofbags)])
    y_pieces_storagebin.append(y_tr[(i*len(y_tr)//amountofbags):(i+1)*(len(y_tr)//amountofbags)])
    
  x_pieces_storagebin=np.asarray(x_pieces_storagebin) 
  y_pieces_storagebin=np.asarray(y_pieces_storagebin) 
    
  return x_pieces_storagebin ,  y_pieces_storagebin

def runmultiplemodels(x_tr = None, y_tr = None, amountofbags = 3, x_val = None, y_val = None, model = False, epochss = 6, batch_sizee = 32, ReUseModel = False):
  if model == False:
    model = neural_net()
 
  if (x_tr and y_tr) == None:
    x_tr , y_tr = bagging(amountofbags = amountofbags)
    
  if (x_val and y_val) == None:
    x_val, y_val = gen_dataset(val_size, N_CLASSES, invoices, targets, tokenizer)
  
  #prob. not necessary
  x_tr = np.array(x_tr)      
  y_tr = np.array(y_tr)
  x_val = np.array(x_val)      
  y_val = np.array(y_val)
  
  print(x_tr.shape)
  for i in range(0,amountofbags):
    print(i)
    model.fit(x_tr[i],y_tr[i],batch_size=batch_sizee,epochs=epochss,validation_data=(x_val,y_val))
    model.save("model"+str(i)+".h5")
    if ReUseModel == False:
      model = neural_net()

## Run a couple of models in order to take their average later
runmultiplemodels()

"""## Generate Demo Invoice"""

'''
To make predictions from our model, we need to create 
sequences around every character from the invoice.

We the making predictions for every charater based on their invoice
'''

# Choose a random invoice:
index = np.random.randint(0,len(invoices))
#index = 100
inv = invoices[index]
tar = targets[index]


chars = [] # Holds the individual characters
data = [] # Holds the sequences around the characters
y_true = [] # Holds the true targets for each character

# Loop over cha racters indices
for i in range(len(inv) -1):
    # Create sequence around this character
    x,y = gen_sub(inv,tar,PADDING,m=i)
    # Tokenize the sequence to one hot
    xm = tokenizer.texts_to_matrix(x)
    # Get the character itself
    c = inv[i]
    
    chars.append(c)
    data.append(xm)
    y_true.append(y)

# For demo purposes we can look what our invoice looks like
df = pd.DataFrame({'Char':chars,'Target':y_true})
dfc = pd.DataFrame({'Char':chars,'Target':y_true})

# Show all characters belonging to the amount
df[df.Target == 5]

## Create test data for predictions with neural net
x_test = np.array(data)

"""## Making Predictions"""

#Takes a while.
y_pred = []
modellist = []
for i in range(0,3):
  filename = "model"+str(i)+".h5"
  modellist.append(load_model("model"+str(i)+".h5"))
  y_pred.append(modellist[i].predict(x_test))

## These are relatively large files & the models have already been used to predict values so we can delete them
!ls

!rm -r model0.h5
!rm -r model1.h5
!rm -r model2.h5
!rm -r model3.h5
!rm -r model4.h5
!rm -r model5.h5
!rm -r modeltest.h5

##Transform predictions into array format
y_pred=np.asarray(y_pred)

#Take average of mean and median
y_pred_mean=np.zeros(shape=(np.size(y_pred,1),np.size(y_pred,2)))

for i in list(range(0,(len(y_pred)-1))):
  y_pred_mean+=y_pred[i]/len(y_pred)

#Convolves over each weight vector (in {0-5}) (i.e. prior to taking argmax). (this convolution removes islands (both noise and false positives))

def OneDConvFive(y_preconv):
    # Repeatedly convolving blocks effectively results in a gaussianshaped filter with a rather large sigma
    gausslikefiltercoeffs = [0.05,1,1,1,0.05]
    gauss_size = np.dot(gausslikefiltercoeffs, gausslikefiltercoeffs)
    y_pred_conv = []
    #Dot product between weights and indices can be taken. Use modulo so that there's no index out of range problems
    for i in list(range(0,len(y_preconv))):  
      y_pred_conv.append((gausslikefiltercoeffs[0]*y_preconv[(i-2)%len(y_preconv)]+gausslikefiltercoeffs[1]*y_preconv[(i-1)%len(y_preconv)]+gausslikefiltercoeffs[2]*y_preconv[i%len(y_preconv)]+gausslikefiltercoeffs[3]*y_preconv[(i+1)%len(y_preconv)]+gausslikefiltercoeffs[4]*y_preconv[(i+2)%len(y_preconv)])/gauss_size)
    
    y_pred_conv=np.asarray(y_pred_conv)

    return y_pred_conv

y_pred_convo_mean=OneDConvFive(y_pred_mean)
for i in list(range(0,20)):
    y_pred_convo_mean=OneDConvFive(y_pred_convo_mean)

#Use a copy to get pointers straight 
import copy

y_pred_convo_mean_scaled = copy.copy(y_pred_convo_mean)
y_pred_convo_mean_scaled[:,0]=y_pred_convo_mean_scaled[:,0]*1.5

## Take class with highest predicted probability
y_pred_c=y_pred_convo_mean.argmax(axis=1)
y_pred_c_scaled=y_pred_convo_mean_scaled.argmax(axis=1)
y_pred=y_pred_mean.argmax(axis=1)

#see if a maxvalue / bank account can be found. If so; remove all indices found by machine learning and insert indices at the found locations.
def cheating(invoices, index, y_pred_c_scaled):
  #load invoice data
  inv = invoices[index]
 
  #create new data vector
  y_pred_c_scaled_re=copy.copy(y_pred_c_scaled)
  
  #perform algorithm for total money owed (first find strings of all monetary values, then convert to double, find the maximum value,
  # make sure that the final string contains two decimal numbers.
  foundvalues=re.findall(r'[0-9]*[\.][0-9]{2}', invoices[index])

  maxvalue=max(map(float,foundvalues))
  maxvalue = "%.2f" % maxvalue

  indicesmoninfin = re.search(str(maxvalue), invoices[index])
  indicesmonall = list(range(indicesmoninfin.start(),indicesmoninfin.end()))

  y_pred_c_scaled_re=copy.copy(y_pred_c_scaled)
  if len(maxvalue) > 0:
    for i in list(range(0,len(y_pred_c_scaled_re)-1)):
      if y_pred_c_scaled_re[i]== 5:
        y_pred_c_scaled_re[i] == 0
      
    y_pred_c_scaled_re[indicesmonall] = 5

  #perform algorithm for IBAN
  bankaccount = re.findall(r'[A-Z]{2}[0-9]{2}[A-Z]{4}[0-9]{10}',invoices[index])

  if len(bankaccount) > 0:
    indicesinfinbank = re.search(str(bankaccount[0]),string = invoices[index])
    allindicesbank = list(range(indicesinfinbank.start(),indicesinfinbank.end()))

    for i in list(range(0,len(y_pred_c_scaled)-1)):
      if y_pred_c_scaled_re[i]== 3:
        y_pred_c_scaled_re[i] == 0
        
    y_pred_c_scaled_re[allindicesbank] = 3
      
  return y_pred_c_scaled_re

y_pred_c_scaled_re = cheating(invoices, index, y_pred_c_scaled)

#Create dataframe to check accuracy
df['Predicted'] = y_pred
df['Predicted_C'] = y_pred_c
df['Predicted_C_Scaled'] = y_pred_c_scaled
df['Predicted_C_Scaled_Regexp'] = y_pred_c_scaled_re

## Check accuracy for a single invoice

for i in range(0,6):
  if len(df[df.Target == i]) != 0:
    placeholder = df[df.Target == i]
    base_acc = (sum(placeholder.Target == placeholder.Predicted) / len(placeholder.Target))*100
    final_acc = (sum(placeholder.Target == placeholder.Predicted_C_Scaled_Regexp) / len(placeholder.Target))*100
    print("The baseline model achieves an accuracy of {} % for class {} . Our model achieves an accuracy of {} %".format(base_acc,i,final_acc))

finalacc_base = (sum(df.Target == df.Predicted) / len(df.Target))*100
finalacc_our = (sum(df.Target == df.Predicted_C_Scaled_Regexp) / len(df.Target))*100

print("The baseline model achieves an accuracy of {} % for all classes. Our model achieves an accuracy of {} %".format(finalacc_base,finalacc_our))

## Final comments:
## As of now, the accuracy is calculated only over one (randomly sampled) invoice. 
## We still could write a general code that does that for all the invoices.
## If you want to find out accuracy over a new (randomly sampled) invoice, re-run code from "runmultiplemodels()" (the box before "Generating Demo Invoice")
